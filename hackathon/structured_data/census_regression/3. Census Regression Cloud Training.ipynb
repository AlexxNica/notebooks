{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"about\"></a>\n",
    "About this notebook\n",
    "======\n",
    "\n",
    "This notebook assumes you have ran the local Census Regression notebook and you have not deleted the LOCAL_ROOT folder. In this notebook, we will use train a Tensorflow model on CloudML. This notebook will does not assume that the notebook \"2. Census Regression Cloud Preprocessing\" was executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "Setting things up\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/pydatalab/solutionbox/structured_data/dist/structured_data-0.0.1.tar.gz\n",
      "Collecting tensorflow==1.0 (from structured-data==0.0.1)\n",
      "  Using cached tensorflow-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting protobuf==3.1.0 (from structured-data==0.0.1)\n",
      "  Using cached protobuf-3.1.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-dataflow==0.5.5 (from structured-data==0.0.1)\n",
      "Collecting numpy>=1.11.0 (from tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting wheel (from tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached wheel-0.29.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf==3.1.0->structured-data==0.0.1)\n",
      "  Using cached setuptools-34.2.0-py2.py3-none-any.whl\n",
      "Collecting protorpc<0.12,>=0.9.1 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "  Using cached protorpc-0.11.1-py2-none-any.whl\n",
      "Collecting avro<2.0.0,>=1.7.7 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting crcmod<2.0,>=1.7 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting google-apitools<1.0.0,>=0.5.6 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "  Using cached google_apitools-0.5.7-py2-none-any.whl\n",
      "Collecting python-gflags<4.0.0,>=2.0 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting oauth2client<4.0.0,>=2.0.1 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting httplib2<0.10,>=0.8 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting googledatastore==6.4.1 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting pyyaml<4.0.0,>=3.10 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting dill<0.3,>=0.2.5 (from google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow==1.0->structured-data==0.0.1)\n",
      "  Using cached pbr-1.10.0-py2.py3-none-any.whl\n",
      "Collecting packaging>=16.8 (from setuptools->protobuf==3.1.0->structured-data==0.0.1)\n",
      "  Using cached packaging-16.8-py2.py3-none-any.whl\n",
      "Collecting appdirs>=1.4.0 (from setuptools->protobuf==3.1.0->structured-data==0.0.1)\n",
      "  Using cached appdirs-1.4.0-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from oauth2client<4.0.0,>=2.0.1->google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "  Using cached rsa-3.4.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client<4.0.0,>=2.0.1->google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "  Using cached pyasn1-0.2.2-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client<4.0.0,>=2.0.1->google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "  Using cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting proto-google-datastore-v1==1.3.1 (from googledatastore==6.4.1->google-cloud-dataflow==0.5.5->structured-data==0.0.1)\n",
      "Collecting pyparsing (from packaging>=16.8->setuptools->protobuf==3.1.0->structured-data==0.0.1)\n",
      "  Using cached pyparsing-2.1.10-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: structured-data\n",
      "  Running setup.py bdist_wheel for structured-data ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/93/76/9c/ffc7ce503d47e9b9a7e57f17b633f1ba32b49e265f0c004570\n",
      "Successfully built structured-data\n",
      "Installing collected packages: numpy, funcsigs, six, pbr, mock, wheel, pyparsing, packaging, appdirs, setuptools, protobuf, tensorflow, protorpc, avro, crcmod, httplib2, pyasn1, rsa, pyasn1-modules, oauth2client, google-apitools, python-gflags, proto-google-datastore-v1, googledatastore, pyyaml, dill, google-cloud-dataflow, structured-data\n",
      "  Found existing installation: numpy 1.12.0\n",
      "    Uninstalling numpy-1.12.0:\n",
      "      Successfully uninstalled numpy-1.12.0\n",
      "  Found existing installation: funcsigs 1.0.2\n",
      "    Uninstalling funcsigs-1.0.2:\n",
      "      Successfully uninstalled funcsigs-1.0.2\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: pbr 1.10.0\n",
      "    Uninstalling pbr-1.10.0:\n",
      "      Successfully uninstalled pbr-1.10.0\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: wheel 0.29.0\n",
      "    Uninstalling wheel-0.29.0:\n",
      "      Successfully uninstalled wheel-0.29.0\n",
      "  Found existing installation: pyparsing 2.1.10\n",
      "    Uninstalling pyparsing-2.1.10:\n",
      "      Successfully uninstalled pyparsing-2.1.10\n",
      "  Found existing installation: packaging 16.8\n",
      "    Uninstalling packaging-16.8:\n",
      "      Successfully uninstalled packaging-16.8\n",
      "  Found existing installation: appdirs 1.4.0\n",
      "    Uninstalling appdirs-1.4.0:\n",
      "      Successfully uninstalled appdirs-1.4.0\n",
      "  Found existing installation: setuptools 34.2.0\n",
      "    Uninstalling setuptools-34.2.0:\n",
      "      Successfully uninstalled setuptools-34.2.0\n",
      "  Found existing installation: protobuf 3.1.0\n",
      "    Uninstalling protobuf-3.1.0:\n",
      "      Successfully uninstalled protobuf-3.1.0\n",
      "  Found existing installation: tensorflow 1.0.0\n",
      "    Uninstalling tensorflow-1.0.0:\n",
      "      Successfully uninstalled tensorflow-1.0.0\n",
      "  Found existing installation: protorpc 0.11.1\n",
      "    Uninstalling protorpc-0.11.1:\n",
      "      Successfully uninstalled protorpc-0.11.1\n",
      "  Found existing installation: avro 1.8.1\n",
      "    Uninstalling avro-1.8.1:\n",
      "      Successfully uninstalled avro-1.8.1\n",
      "  Found existing installation: crcmod 1.7\n",
      "    Uninstalling crcmod-1.7:\n",
      "      Successfully uninstalled crcmod-1.7\n",
      "  Found existing installation: httplib2 0.9.2\n",
      "    Uninstalling httplib2-0.9.2:\n",
      "      Successfully uninstalled httplib2-0.9.2\n",
      "  Found existing installation: pyasn1 0.2.2\n",
      "    Uninstalling pyasn1-0.2.2:\n",
      "      Successfully uninstalled pyasn1-0.2.2\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.0.8\n",
      "    Uninstalling pyasn1-modules-0.0.8:\n",
      "      Successfully uninstalled pyasn1-modules-0.0.8\n",
      "  Found existing installation: oauth2client 3.0.0\n",
      "    Uninstalling oauth2client-3.0.0:\n",
      "      Successfully uninstalled oauth2client-3.0.0\n",
      "  Found existing installation: google-apitools 0.5.7\n",
      "    Uninstalling google-apitools-0.5.7:\n",
      "      Successfully uninstalled google-apitools-0.5.7\n",
      "  Found existing installation: python-gflags 3.1.1\n",
      "    Uninstalling python-gflags-3.1.1:\n",
      "      Successfully uninstalled python-gflags-3.1.1\n",
      "  Found existing installation: proto-google-datastore-v1 1.3.1\n",
      "    Uninstalling proto-google-datastore-v1-1.3.1:\n",
      "      Successfully uninstalled proto-google-datastore-v1-1.3.1\n",
      "  Found existing installation: googledatastore 6.4.1\n",
      "    Uninstalling googledatastore-6.4.1:\n",
      "      Successfully uninstalled googledatastore-6.4.1\n",
      "  Found existing installation: PyYAML 3.12\n",
      "    Uninstalling PyYAML-3.12:\n",
      "      Successfully uninstalled PyYAML-3.12\n",
      "  Found existing installation: dill 0.2.6\n",
      "    Uninstalling dill-0.2.6:\n",
      "      Successfully uninstalled dill-0.2.6\n",
      "  Found existing installation: google-cloud-dataflow 0.5.5\n",
      "    Uninstalling google-cloud-dataflow-0.5.5:\n",
      "      Successfully uninstalled google-cloud-dataflow-0.5.5\n",
      "  Found existing installation: structured-data 0.0.1\n",
      "    Uninstalling structured-data-0.0.1:\n",
      "      Successfully uninstalled structured-data-0.0.1\n",
      "Successfully installed appdirs-1.4.0 avro-1.8.1 crcmod-1.7 dill-0.2.6 funcsigs-1.0.2 google-apitools-0.5.7 google-cloud-dataflow-0.5.5 googledatastore-6.4.1 httplib2-0.9.2 mock-2.0.0 numpy-1.12.0 oauth2client-3.0.0 packaging-16.8 pbr-1.10.0 proto-google-datastore-v1-1.3.1 protobuf-3.1.0 protorpc-0.11.1 pyasn1-0.2.2 pyasn1-modules-0.0.8 pyparsing-2.1.10 python-gflags-3.1.1 pyyaml-3.12 rsa-3.4.2 setuptools-34.2.0 six-1.10.0 structured-data-0.0.1 tensorflow-1.0.0 wheel-0.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall /content/pydatalab/solutionbox/structured_data/dist/structured_data-0.0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datalab_solutions.structured_data as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the versions of structured_data and TF we have. Make sure TF is 1.0.0, and SD is 0.0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 1.0.0\n",
      "sd 0.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import datalab.mlalpha as mlalpha\n",
    "\n",
    "print('tf ' + str(tf.__version__))\n",
    "print('sd ' + str(sd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will write files during preprocessing, training, and prediction. Please give a root folder you wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-ml-dev-census-regression-datalab/eval_data.csv#1487810748184569...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/predict_data.csv#1487810748207107...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/schema.json#1487810749604878...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/train_data.csv#1487810748313783...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/transforms.json#1487810750781897...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/numerical_analysis.json#1487810752321057...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/schema.json#1487810752196173...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_AGEP.csv#1487810752311137...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_COW.csv#1487810752273032...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_ESP.csv#1487810752283117...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_ESR.csv#1487810752288921...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_FOD1P.csv#1487810752245429...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_JWMNP.csv#1487810752331688...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_HINS4.csv#1487810752268851...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_INDP.csv#1487810752292186...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_JWTR.csv#1487810752258245...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_MAR.csv#1487810752287377...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_POWPUMA.csv#1487810752305544...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_PUMA.csv#1487810752276974...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_RAC1P.csv#1487810752255722...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SCHL.csv#1487810752279974...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SCIENGRLP.csv#1487810752214753...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SERIALNO.csv#1487810752258331...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SEX.csv#1487810752244339...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_WKW.csv#1487810754833666...\n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/training/staging/sd.tar.gz#1487810766172632...\n",
      "/ [26/26 objects] 100% Done                                                     \n",
      "Operation completed over 26 objects.                                             \n",
      "Removing gs://cloud-ml-dev-census-regression-datalab/...\n",
      "Creating gs://cloud-ml-dev-census-regression-datalab/...\n"
     ]
    }
   ],
   "source": [
    "LOCAL_ROOT = './census_regression_workspace' # This should be the same as what was used in the local census notebook\n",
    "CLOUD_ROOT = 'gs://' + datalab_project_id() + '-census-regression-datalab'\n",
    "\n",
    "if not file_io.file_exists(LOCAL_ROOT):\n",
    "  raise ValueErro('LOCAL_ROOT not found. Did you run the local notebook?')\n",
    "!gsutil mb {CLOUD_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us put the csv files on GCS and the output of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./census_regression_workspace/train_data.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/eval_data.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/predict_data.csv [Content-Type=text/csv]...\n",
      "/ [3/3 files][200.1 KiB/200.1 KiB] 100% Done                                    \n",
      "Operation completed over 3 objects/200.1 KiB.                                    \n",
      "Copying file://./census_regression_workspace/schema.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n",
      "Copying file://./census_regression_workspace/transforms.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.0 KiB/  1.0 KiB]                                                \n",
      "Operation completed over 1 objects/1.0 KiB.                                      \n",
      "Copying file://./census_regression_workspace/preprocess/vocab_SCIENGRLP.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_SERIALNO.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_SCHL.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_ESR.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_HINS4.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_COW.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_AGEP.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_PUMA.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_ESP.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_JWTR.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_SEX.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_INDP.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_RAC1P.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_FOD1P.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/schema.json [Content-Type=application/json]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_POWPUMA.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_WKW.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_JWMNP.csv [Content-Type=text/csv]...\n",
      "Copying file://./census_regression_workspace/preprocess/numerical_analysis.json [Content-Type=application/json]...\n",
      "Copying file://./census_regression_workspace/preprocess/vocab_MAR.csv [Content-Type=text/csv]...\n",
      "\\\n",
      "Operation completed over 20 objects/17.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp {os.path.join(LOCAL_ROOT, '*_data.csv')} {CLOUD_ROOT}\n",
    "!gsutil cp {os.path.join(LOCAL_ROOT, 'schema.json')} {CLOUD_ROOT}\n",
    "!gsutil cp {os.path.join(LOCAL_ROOT, 'transforms.json')} {CLOUD_ROOT}\n",
    "!gsutil -m cp -r {os.path.join(LOCAL_ROOT, 'preprocess')} {CLOUD_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/numerical_analysis.json\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/schema.json\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_AGEP.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_COW.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_ESP.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_ESR.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_FOD1P.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_HINS4.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_INDP.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_JWMNP.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_JWTR.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_MAR.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_POWPUMA.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_PUMA.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_RAC1P.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SCHL.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SCIENGRLP.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SERIALNO.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_SEX.csv\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/preprocess/vocab_WKW.csv\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {CLOUD_ROOT}/preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local_preprocessing\"></a>\n",
    "Cloudml Training\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-ml-dev-census-regression-datalab/trainingjunk/staging/sd.tar.gz#1487811680804281...\n",
      "/ [1/1 objects] 100% Done                                                       \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -r {CLOUD_ROOT}/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_csv = mlalpha.CsvDataSet(\n",
    "  file_pattern=os.path.join(CLOUD_ROOT, 'train_data.csv'),\n",
    "  schema_file=os.path.join(CLOUD_ROOT, 'schema.json'))\n",
    "eval_csv = mlalpha.CsvDataSet(\n",
    "  file_pattern=os.path.join(CLOUD_ROOT, 'eval_data.csv'),\n",
    "  schema_file=os.path.join(CLOUD_ROOT, 'schema.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctc = mlalpha.CloudTrainingConfig(\n",
    "  region='us-central1',\n",
    "  scale_tier='STANDARD_1' #See https://cloud.google.com/ml/reference/rest/v1beta1/projects.jobs#ScaleTier\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building package and uploading to gs://cloud-ml-dev-census-regression-datalab/trainingjunk2/staging/sd.tar.gz\n",
      "Job request send. View status of job at\n",
      "https://console.developers.google.com/ml/jobs?project=cloud-ml-dev\n",
      "createTime: '2017-02-23T01:01:40Z'\n",
      "jobId: structured_data_train_170223_010140\n",
      "state: QUEUED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --train_data_paths=gs://cloud-ml-dev-census-regression-datalab/train_data.csv\n",
      "  - --eval_data_paths=gs://cloud-ml-dev-census-regression-datalab/eval_data.csv\n",
      "  - --output_path=gs://cloud-ml-dev-census-regression-datalab/trainingjunk2\n",
      "  - --preprocess_output_dir=gs://cloud-ml-dev-census-regression-datalab/preprocess\n",
      "  - --transforms_file=gs://cloud-ml-dev-census-regression-datalab/transforms.json\n",
      "  - --model_type=dnn_regression\n",
      "  - --max_steps=2000\n",
      "  - --train_batch_size=100\n",
      "  - --eval_batch_size=100\n",
      "  - --min_eval_frequency=100\n",
      "  - --learning_rate=0.01\n",
      "  - --epsilon=0.0005\n",
      "  - --layer_size1=5\n",
      "  - --layer_size2=5\n",
      "  - --layer_size3=5\n",
      "  packageUris:\n",
      "  - gs://cloud-ml-dev-census-regression-datalab/trainingjunk2/staging/sd.tar.gz\n",
      "  pythonModule: datalab_solutions.structured_data.trainer.task\n",
      "  region: us-central1\n",
      "  scaleTier: STANDARD_1\n",
      "trainingOutput: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = sd.cloud_train(\n",
    "  train_dataset=train_csv,\n",
    "  eval_dataset=eval_csv,\n",
    "  transforms=os.path.join(CLOUD_ROOT, 'transforms.json'),\n",
    "  preprocess_output_dir=os.path.join(CLOUD_ROOT, 'preprocess'),\n",
    "  output_dir=os.path.join(CLOUD_ROOT, 'training'),\n",
    "  model_type='dnn_regression',\n",
    "  max_steps=2000,\n",
    "  layer_sizes=[5, 5, 5],\n",
    "  cloud_training_config=ctc,\n",
    ")\n",
    "job.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training is done, {CLOUD_ROOT}/training should contain the folders train, model, evaluation_model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-dev-census-regression-datalab/training/evaluation_model/\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/training/model/\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/training/staging/\r\n",
      "gs://cloud-ml-dev-census-regression-datalab/training/train/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls  {CLOUD_ROOT}/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning things up\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to delete the files you made on GCS, uncomment and run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!gsutil rm -fr {CLOUD_ROOT}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
