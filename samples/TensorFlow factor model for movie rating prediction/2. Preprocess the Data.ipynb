{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "Based on our analysis in the previous notebook, we identified several things needing consideration. The biggest one being that about 5% of users account for a third of the rating data. We will simply remove these high rating-count users from the dataset in this preprocessing file. Movies with less than 10 ratings are also removed. Lastly, we divide the datasets into two csv files: one for training and another for evaluating the model. The files will be saved to a folder called 'embeddingModel' in the location of this notebook. \n",
    "\n",
    "This notebook assumes the Movielens data was downloaded and unzipped by the [previous notebook](1. Downloading and Exploring the Movielens Data.ipynb) in this series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_RATING_FILE = 'ml-10M100K/ratings.dat'\n",
    "\n",
    "OUTPUT_DIR = 'embeddingModel'\n",
    "WALS_OUTPUT_TRAIN_FILE = os.path.join(OUTPUT_DIR, 'walsMovielensTrain.csv')\n",
    "WALS_OUTPUT_TEST_FILE = os.path.join(OUTPUT_DIR, 'walsMovielensTest.csv')\n",
    "WALS_OUTPUT_STATS_FILE = os.path.join(OUTPUT_DIR,'matrixInfo.yaml')\n",
    "\n",
    "# Preprocessing parameters\n",
    "TRANSLATION = 3.5\n",
    "MAX_USER_RATING_THRESHOLD = 500\n",
    "MIN_MOVIE_RATING_THRESHOLD = 10\n",
    "PERCENT_TRAIN = 0.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for the WALS model\n",
    "\n",
    "Input to the WALS model is a sparse matrix. We convert the movie lens data of 'userid', 'movieid', 'rating' into a 'matrix row index', 'matrix column index' and 'shifted rating'. The matrix indices start at zero. The WALS model performs better when the true mean rating is close to zero, so we subtract 3.5 from each rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv(INPUT_RATING_FILE, sep='::', header=None, names=['userid', 'movieid', 'rating', 'timestamp'], usecols=['userid', 'movieid', 'rating'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset stats: 69878 users, 10677 movies, 10000054 ratings\n",
      "New raw stats: 66219 users, 9670 movies, 6896233 ratings\n",
      "Removed 1007 movies, or 9.431488% percent of all movies\n",
      "Removed 3659 users, or 5.236269% percent of all users\n",
      "Removed 3103821 ratings, or 31.038042% percent of all ratings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_userids = raw['userid'].unique()\n",
    "unique_movieids = raw['movieid'].unique()\n",
    "num_unique_users = unique_userids.shape[0]\n",
    "num_unique_movies = unique_movieids.shape[0]\n",
    "num_ratings = raw.shape[0]\n",
    "print('Raw dataset stats: %d users, %d movies, %d ratings' % (num_unique_users, num_unique_movies, num_ratings))\n",
    "\n",
    "user_histo = raw['userid'].value_counts()\n",
    "blacklist_users_index = (user_histo.values > MAX_USER_RATING_THRESHOLD)\n",
    "blacklist_users = user_histo.index[blacklist_users_index]\n",
    "whitelist_users = user_histo.index[np.logical_not(blacklist_users_index)]\n",
    "\n",
    "movie_histo = raw['movieid'].value_counts()\n",
    "blacklist_movies_index = (movie_histo.values < MIN_MOVIE_RATING_THRESHOLD)\n",
    "blacklist_movies = movie_histo.index[blacklist_movies_index]\n",
    "whitelist_movies = movie_histo.index[np.logical_not(blacklist_movies_index)]\n",
    "\n",
    "delete_mask1 = raw['userid'].isin(blacklist_users)\n",
    "delete_mask2 = raw['movieid'].isin(blacklist_movies)\n",
    "delete_mask = delete_mask1 | delete_mask2\n",
    "raw = raw[~delete_mask]\n",
    "\n",
    "unique_userids = raw['userid'].unique()\n",
    "unique_movieids = raw['movieid'].unique()\n",
    "\n",
    "new_num_unique_users = unique_userids.shape[0]\n",
    "new_num_unique_movies = unique_movieids.shape[0]\n",
    "new_num_ratings = raw.shape[0]\n",
    "\n",
    "print 'New raw stats: %d users, %d movies, %d ratings' % (new_num_unique_users, new_num_unique_movies, new_num_ratings)\n",
    "print 'Removed %d movies, or %f%% percent of all movies' % (num_unique_movies - new_num_unique_movies, 100*(num_unique_movies - new_num_unique_movies)/float(num_unique_movies))\n",
    "print 'Removed %d users, or %f%% percent of all users' % (num_unique_users - new_num_unique_users, 100*(num_unique_users - new_num_unique_users)/float(num_unique_users))\n",
    "print 'Removed %d ratings, or %f%% percent of all ratings' % (num_ratings - new_num_ratings, 100*(num_ratings - new_num_ratings)/float(num_ratings))\n",
    "\n",
    "\n",
    "# Continuously reindex the user and movie id's to start at 0.\n",
    "\n",
    "i = 0\n",
    "old_userid_to_new = {}\n",
    "for uid in unique_userids:\n",
    "  old_userid_to_new[uid] = i\n",
    "  i += 1\n",
    "\n",
    "row = raw['userid'].apply(lambda x : old_userid_to_new[x])\n",
    "\n",
    "i = 0\n",
    "old_movieid_to_new = {}\n",
    "for mid in unique_movieids:\n",
    "  old_movieid_to_new[mid] = i\n",
    "  i += 1\n",
    "\n",
    "col = raw['movieid'].apply(lambda x : old_movieid_to_new[x])\n",
    "\n",
    "\n",
    "values = raw['rating'].as_matrix().astype(np.float32) - TRANSLATION\n",
    "\n",
    "preprocessed = pd.DataFrame(columns=['userid', 'movieid', 'rating'])\n",
    "preprocessed['userid'] = row\n",
    "preprocessed['movieid'] = col\n",
    "preprocessed['rating'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def splitValues(df):\n",
    "  '''Given a dataframe, splits the rows into two dataframes\n",
    "  \n",
    "  Returns:\n",
    "    df_train: a dataset for training, containing PERCENT_TRAIN\n",
    "      of the rows of df.\n",
    "    df_test: a dataset for testing.\n",
    "  '''\n",
    "  \n",
    "  num_items = raw.shape[0]\n",
    "\n",
    "  # Get a permutation of the row indices of df\n",
    "  random.seed(34512)\n",
    "  index = np.random.permutation(num_items)\n",
    "\n",
    "  num_train = int(num_items * PERCENT_TRAIN)\n",
    "\n",
    "  df_train = df.iloc[index[:num_train], :]\n",
    "  df_test = df.iloc[index[num_train:], :]\n",
    "  \n",
    "  return (df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before start writing files out, make sure folder exists.\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data for training and testing, and save to a file.\n",
    "(preprocessed_train, preprocessed_test) = splitValues(preprocessed)\n",
    "preprocessed_train.to_csv(WALS_OUTPUT_TRAIN_FILE, header=False, index=False)\n",
    "preprocessed_test.to_csv(WALS_OUTPUT_TEST_FILE, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the number of rows and columns to a yaml file for easy use.\n",
    "with open(WALS_OUTPUT_STATS_FILE, 'w') as outfile:\n",
    "    matrix_info = {'num_rows': new_num_unique_users, 'num_columns': new_num_unique_movies}\n",
    "    yaml.dump(matrix_info, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
