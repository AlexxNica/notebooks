{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): geopy in /usr/local/lib/python2.7/dist-packages\n",
      "Processing /content/cloudml.tar.gz\n",
      "Requirement already up-to-date: oauth2client==2.2.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: protobuf>=3.0.0b2.post2 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: google-cloud-dataflow>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: bs4>=0.0.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: numpy>=1.10.4 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: pillow>=3.2.0 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: dpkt>=1.8.7 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: nltk>=3.2.1 in /usr/local/lib/python2.7/dist-packages (from cloudml==0.1.2)\n",
      "Requirement already up-to-date: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client==2.2.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.0.0b2.post2->cloudml==0.1.2)\n",
      "Requirement already up-to-date: google-apitools>=0.5.2 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: dill>=0.2.5 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: protorpc>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: mock>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: avro>=1.7.7 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: python-gflags>=2.0 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pyyaml>=3.10 in /usr/local/lib/python2.7/dist-packages (from google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: beautifulsoup4 in /usr/local/lib/python2.7/dist-packages (from bs4>=0.0.1->cloudml==0.1.2)\n",
      "Requirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Requirement already up-to-date: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=1.0.1->google-cloud-dataflow>=0.4.0->cloudml==0.1.2)\n",
      "Installing collected packages: cloudml\n",
      "  Found existing installation: cloudml 0.1.2\n",
      "    Uninstalling cloudml-0.1.2:\n",
      "      Successfully uninstalled cloudml-0.1.2\n",
      "  Running setup.py install for cloudml ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25hSuccessfully installed cloudml-0.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade /content/cloudml.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import google.cloud.ml as ml\n",
    "from google.cloud.ml.dataflow.io import tfrecordio\n",
    "import apache_beam as beam\n",
    "\n",
    "data_dir = '/content/preprocessing_demo/geolocation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we explain how a user can introduce her own feature type. The Running Example is a Geo-Location Feature (latitute, longitude)\n",
    "\n",
    "For this example a helper function is needed, about figuring out if a point belongs in a polygon. You can consider this an external dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for geo-location data operations.\"\"\"\n",
    "\n",
    "def point_in_polygon(polygon_points, p):\n",
    "  \"\"\"Point in polygon function, Returs true if p is inside the polygon.\n",
    "\n",
    "    Use the Ray Casting Method to determine if a point belongs in a polygon.\n",
    "    Copy translated from C code from here\n",
    "    google3/third_party/py/skimage/_shared/geometry.pyx\n",
    "\n",
    "    Args:\n",
    "      polygon_points: list of tuples corresponding to the coordinates of\n",
    "      vertices. They must be in clock-wise or counter-clockwise order.\n",
    "      p: tuple corresponding to the coordinates of point to check.\n",
    "    Returns:\n",
    "      True if point in polygon.\n",
    "  \"\"\"\n",
    "  x, y = p\n",
    "  result = False\n",
    "  for i in range(len(polygon_points)):\n",
    "    if p == polygon_points[i]:  # if point is exactly on edge, count as true\n",
    "      return True\n",
    "    j = (i + 1) % len(polygon_points)  # next point\n",
    "    xp1, yp1 = polygon_points[i]\n",
    "    xp2, yp2 = polygon_points[j]\n",
    "    if yp2 == yp1:\n",
    "      x_intersect = float('+inf')\n",
    "    else:\n",
    "      x_intersect = (xp2 - xp1) * (y - yp1) / (yp2 - yp1) + xp1\n",
    "    if (y <= max(yp1, yp2) and y >= min(yp1, yp2)) and (x <= x_intersect):\n",
    "      result = not result\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, user defines a **Feature Column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import google.cloud.ml.features as features\n",
    "\n",
    "class GeoFeatureColumn(features.FeatureColumn):\n",
    "  \"\"\"Represents a feature column for geolocation.\n",
    "\n",
    "    Essentially it is a longtitude-latitude value. The column, also receives a\n",
    "    path as input, which is where the polygon information is stored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name, path, make_categorical=False):\n",
    "    super(GeoFeatureColumn, self).__init__(name, 'geo-location')\n",
    "    polygons = self._load_polygons(path)\n",
    "    self._set_transformation('reverse-geo-code',\n",
    "                             {'polygons': polygons,\n",
    "                              'make_categorical': make_categorical})\n",
    "\n",
    "  def _load_polygons(self, path):\n",
    "    polygons = []\n",
    "    with open(path, 'r') as polygon_file:\n",
    "      for line in polygon_file:\n",
    "        if line.startswith('\"MULTIPOLYGON'):\n",
    "          points = re.findall(r'[\\-0-9\\.]+ [\\-0-9\\.]+', line)\n",
    "          # turn points into list of tuples\n",
    "          points = [tuple(map(float, x.split())) for x in points]\n",
    "          polygon_info = line.split(')))\",')[1].split(',')\n",
    "          polygons.append({'points': points, 'info': polygon_info})\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a user *optionally* defines an Analyzer (example without analyzer in the Image Sample)\n",
    "In this case the analyzer, a function on the whole feature column. This allows us to create some useful metadata.\n",
    "The metadata defined here are the four edge points that can form a bounding box of all the other locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features._registries as registries\n",
    "import google.cloud.ml.features._analysis as analysis\n",
    "\n",
    "@registries.register_analyzer('geo-location')\n",
    "class GeoColumnAnalyzer(analysis.ColumnAnalyzer):\n",
    "  \"\"\"Analyzer for Geo-Location values.\n",
    "\n",
    "    Calculates total number of datapoints and the four edge points which can\n",
    "    form a bounding box over all the given points. From that, calculates total\n",
    "    area the points cover (in square miles).\n",
    "    In order to perform the analysis, some filtering of 'bad values' must be\n",
    "    done.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, column):\n",
    "    super(GeoColumnAnalyzer, self).__init__(column)\n",
    "    self._aggregator = GeoColumnAnalyzer.Aggregator(self._get_column_metadata())\n",
    "\n",
    "  def apply(self, values):\n",
    "    \"\"\"Filtering of bad input values, and aggregation of column info.\"\"\"   \n",
    "    return (values | beam.Filter(lambda x: x is not None)\n",
    "            | beam.Map('makefloat', lambda x: tuple(map(float, x.split(' '))))\n",
    "            | beam.Filter('remove null', lambda x: x != 0)\n",
    "            | beam.CombineGlobally('Analysis',\n",
    "                                   self._aggregator).without_defaults())\n",
    "  \n",
    "  class Aggregator(beam.core.CombineFn):\n",
    "    \"\"\"Aggregator to combine values within a geo-location column.\n",
    "\n",
    "       Calculates the 4 border points. (Points with min/max latitute and\n",
    "       longtitude). Also calculates the total area covered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column):\n",
    "      self._column = column\n",
    "\n",
    "    def create_accumulator(self):\n",
    "      return ((float('+inf'), float('+inf')), (float('+inf'), float('+inf')),\n",
    "              (float('-inf'), float('-inf')), (float('-inf'), float('-inf')), 0)\n",
    "\n",
    "    def add_input(self, stats, point):\n",
    "      (min_lat, min_lon, max_lat, max_lon, count) = stats\n",
    "      if point is None:\n",
    "        return (min_lat, min_lon, max_lat, max_lon, count)\n",
    "\n",
    "      lat = point[0]\n",
    "      lon = point[1]\n",
    "      if lat < min_lat[0]:\n",
    "        min_lat = point\n",
    "      if lon < min_lon[1]:\n",
    "        min_lon = point\n",
    "      if lat > max_lat[0]:\n",
    "        max_lat = point\n",
    "      if lon > max_lon[1]:\n",
    "        max_lon = point\n",
    "\n",
    "      return (min_lat, min_lon, max_lat, max_lon, count + 1)\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "      min_lats, min_lons, max_lats, max_lons, counts = zip(*accumulators)\n",
    "      min_lat, min_lon, max_lat, max_lon = min_lats[0], min_lons[0], max_lats[\n",
    "          0], max_lons[0]\n",
    "\n",
    "      for p in min_lats:\n",
    "        if p[0] < min_lat[0]:\n",
    "          min_lat = p\n",
    "      for p in max_lats:\n",
    "        if p[0] > max_lat[0]:\n",
    "          max_lat = p\n",
    "      for p in min_lons:\n",
    "        if p[1] < min_lon[1]:\n",
    "          min_lon = p\n",
    "      for p in max_lons:\n",
    "        if p[1] > max_lon[1]:\n",
    "          max_lon = p\n",
    "\n",
    "      return (min_lat, min_lon, max_lat, max_lon, sum(counts))\n",
    "\n",
    "    def extract_output(self, stats):\n",
    "      (min_lat, min_lon, max_lat, max_lon, count) = stats\n",
    "      column = self._column\n",
    "      column['min_lat'] = min_lat\n",
    "      column['min_lon'] = min_lon\n",
    "      column['max_lat'] = max_lat\n",
    "      column['max_lon'] = max_lon\n",
    "      column['count'] = count\n",
    "      column['polygons_dict'] = self._enumerate_polygons(column[\n",
    "          'reverse-geo-code']['polygons'])\n",
    "      return (self._column['name'], column)\n",
    "\n",
    "    def _enumerate_polygons(self, polygons):\n",
    "      polygon_names = set([p['info'][3] for p in polygons ])\n",
    "      return dict(zip(sorted(list(polygon_names)), range(len(polygon_names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a user defines a Transform. \n",
    "This is a function that will run for each value in this column independently, while having the metadata information from the Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features._transforms as transforms\n",
    "import google.cloud.ml.features._registries as registries\n",
    "\n",
    "@registries.register_transformation('geo-location', 'reverse-geo-code')\n",
    "class GeoTransform(transforms.ColumnTransform):\n",
    "  \"\"\"Column Transform Class for geo-location.\n",
    "\n",
    "     Reverse geo-codes lat, lon value into a string based, on a polygon map.\n",
    "  \"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def from_metadata(column):\n",
    "    return GeoTransform(column['min_lat'], column['max_lat'], column['min_lon'],\n",
    "                        column['max_lon'], column['polygons_dict'],\n",
    "                        column['reverse-geo-code']['polygons'],\n",
    "                        column['reverse-geo-code']['make_categorical'])\n",
    "\n",
    "  def __init__(self, min_lat, max_lat, min_lon, max_lon, area, polygons_dict,\n",
    "               polygons, make_categorical):\n",
    "    self._min_lat = min_lat\n",
    "    self._max_lat = max_lat\n",
    "    self._min_lon = min_lon\n",
    "    self._max_lon = max_lon\n",
    "    self._polygons = polygons\n",
    "    self._polygon_dict = polygons_dict\n",
    "    self._make_categorical = make_categorical\n",
    "\n",
    "  def transform(self, value):\n",
    "    point = tuple(map(float, value.split()))\n",
    "    if not self._in_area(point):\n",
    "      cat_str = 'Outside'\n",
    "    else:\n",
    "      cat_str = 'Inside_Unknown'\n",
    "      for polygon in self._polygons:\n",
    "        if point_in_polygon(polygon['points'], point):\n",
    "          cat_str = polygon['info'][3]\n",
    "          break\n",
    "    return self._convert_neighborhood(cat_str)\n",
    "\n",
    "  def _convert_neighborhood(self, cat_str):\n",
    "    if not self._make_categorical:\n",
    "      return [cat_str]\n",
    "    size = len(self._polygon_dict)\n",
    "    cat_array = [0] * size\n",
    "    category = self._polygon_dict.get(cat_str, None)\n",
    "    if category:\n",
    "      cat_array[category] = 1\n",
    "\n",
    "    return cat_array\n",
    "\n",
    "  def _in_area(self, point):\n",
    "    if point[0] < self._min_lat[0] or point[0] > self._max_lat[0]:\n",
    "      return False\n",
    "    if point[1] < self._min_lon[1] or point[1] > self._max_lon[1]:\n",
    "      return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a user can define their own data type and write the methods that want to be applied in that datatype.\n",
    "\n",
    "In order to illustrate an example usage, we go through some code that uses this. We are going to use a (sample of a) publicly avaialble dataset, from NY Taxi Data.\n",
    "\n",
    "First, we define the fields in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.ml.features as features\n",
    "import os\n",
    "POLYGONS_PATH = os.path.join(data_dir, 'ny_polygons.csv')\n",
    "\n",
    "class NYTaxiDataFeatures(object):\n",
    "  \"\"\"Class that defines the features in the NYTaxiDataset.\"\"\"\n",
    "\n",
    "  csv_columns = ('VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime',\n",
    "                 'Store_and_fwd_flag', 'RateCodeID', 'Pickup_longitude',\n",
    "                 'Pickup_latitude', 'Dropoff_longitude', 'Dropoff_latitude',\n",
    "                 'Passenger_count', 'Trip_distance', 'Fare_amount', 'Extra',\n",
    "                 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Ehail_fee',\n",
    "                 'Total_amount', 'Payment_type', 'Trip_type', 'empty', 'empty2',\n",
    "                 'pickup', 'dropoff')\n",
    "\n",
    "  target = features.target('Total_amount').regression()\n",
    "  pickup = GeoFeatureColumn('pickup', POLYGONS_PATH),\n",
    "  dropoff = GeoFeatureColumn('dropoff', POLYGONS_PATH)\n",
    "\n",
    "  other_data = [\n",
    "      features.numeric('Passenger_count'), features.numeric('Trip_distance'),\n",
    "      features.numeric('Tip_amount')\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our preprocessing wrapper. This particular one will print the metadata from the analyzer, and save the pre-processed data into files (so that we can later view them for this demo)\n",
    "We also define some  helper functions to help us print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.io as io\n",
    "def preprocess(pipeline, training_path, testing_path):\n",
    "  feature_set = NYTaxiDataFeatures()\n",
    "\n",
    "  training_data = beam.io.TextFileSource(\n",
    "      training_path,\n",
    "      strip_trailing_newlines=True,\n",
    "      coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "\n",
    "  test_data = beam.io.TextFileSource(\n",
    "      testing_path,\n",
    "      strip_trailing_newlines=True,\n",
    "      coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "\n",
    "  train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "  test = pipeline | beam.Read('ReadTestingData', test_data)\n",
    "\n",
    "  (metadata, train_features, test_features) = (\n",
    "      (train, test) | ml.Preprocess('Preprocess', feature_set))\n",
    "\n",
    "  metadata |= beam.Map('print meta', print_fn_metadata)\n",
    "\n",
    "  train_features_path = write_file(train_features, 'WriteTraining', 'features_train')\n",
    "  test_features_path = write_file(test_features, 'WritePredict', 'features_predict')\n",
    "  return (metadata, train_features_path, test_features_path)\n",
    "  \n",
    "    \n",
    "class ExampleProtoCoder(beam.coders.Coder):\n",
    "  \"\"\"A coder to encode TensorFlow example protos.\n",
    "  \"\"\"\n",
    "\n",
    "  def encode(self, x):\n",
    "    return x.SerializeToString()\n",
    "\n",
    "def write_file(pcollection, label, output_name):\n",
    "  path = os.path.join(data_dir, output_name)\n",
    "\n",
    "  print 'Writing to', path\n",
    "  pcollection | beam.Write(label, tfrecordio.TFRecordSink(\n",
    "      path,\n",
    "      coder = ExampleProtoCoder(),\n",
    "      shard_name_template='',\n",
    "      compression_type=beam.io.fileio.CompressionTypes.ZLIB))\n",
    "\n",
    "  return path\n",
    "\n",
    "def print_results(path):\n",
    "  def parse_example_from_string(serialized):\n",
    "      import tensorflow as tf  # pylint: disable=g-import-not-at-top\n",
    "      example = tf.train.Example()\n",
    "      example.ParseFromString(serialized)\n",
    "      return example\n",
    "\n",
    "  pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "  a = (pipeline | tfrecordio.TFRecordSource( path, compression_type=beam.io.fileio.CompressionTypes.ZLIB)\n",
    "            | 'Deserialize' >> beam.Map(parse_example_from_string))\n",
    "  a |= 'print'>> beam.Map(print_fn_train)\n",
    "  pipeline.run()\n",
    "  return a\n",
    "\n",
    "def print_fn_metadata(values):\n",
    "  print 'Metadata Info'\n",
    "  print values.keys()\n",
    "  print values['columns'].keys()\n",
    "  print values['columns']['pickup'].keys()\n",
    "  print 'min lat:', values['columns']['pickup']['min_lat'],\n",
    "  print 'max_lat:', values['columns']['pickup']['max_lat']\n",
    "  print values['features'].keys()\n",
    "\n",
    "  return values\n",
    "\n",
    "def print_fn_train(values):\n",
    "  a = str(values)\n",
    "  if 'MN13' in a and 'BK73' in a:\n",
    "    print a\n",
    "  return values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our data does not come in the format we want. But its very easy to do some data_preparation steps with dataflow. \n",
    "We define the method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "  \"\"\"Prepare raw data into a format that is ready to be used for preprocessing.\n",
    "\n",
    "     This includes, combining two columns to form a geo-location, filtering out\n",
    "     lines with null input and non full entries.\n",
    "  \"\"\"\n",
    "\n",
    "  output_path = os.path.join(data_dir, 'prepared_data.csv')\n",
    "  training_path = os.path.join(data_dir, 'training_data.csv')\n",
    "  testing_path = os.path.join(data_dir, 'testing_data.csv')\n",
    "\n",
    "  p = beam.Pipeline('DirectPipelineRunner')\n",
    "  data = (p | beam.io.Read(\n",
    "      'ReadFromText',\n",
    "      beam.io.TextFileSource(path)))\n",
    "\n",
    "  data = (data\n",
    "          | beam.Map('split columns', lambda x: x.strip().split(','))\n",
    "          | beam.Filter('remove non full entries', lambda row: len(row) >= 22)\n",
    "          | beam.Filter('remove null locations', is_null_location))\n",
    "\n",
    "  data |= beam.Map('Combine Pickup', concatenate, [5, 6])\n",
    "  data |= beam.Map('Combine Dropoff', concatenate, [7, 8])\n",
    "\n",
    "  # save to file as strings again\n",
    "  _ = (data | beam.Map('Make strings', lambda x: ','.join(x))\n",
    "       | beam.Write(\n",
    "           'Save prepared',\n",
    "           beam.io.TextFileSink(\n",
    "               output_path, shard_name_template='')))\n",
    "  p.run()\n",
    "\n",
    "  training = open(training_path, 'w')\n",
    "  testing = open(testing_path, 'w')\n",
    "  with open(output_path, 'r') as f:\n",
    "    for line in f:\n",
    "      if random.random() < 0.2:\n",
    "        testing.write(line)\n",
    "      else:\n",
    "        training.write(line)\n",
    "\n",
    "  training.close()\n",
    "  testing.close()\n",
    "\n",
    "  return training_path, testing_path\n",
    "\n",
    "def concatenate(data, idx_list):\n",
    "  \"\"\"Concatenates a list of columns, adds it as a new one, at the end of data.\n",
    "  \"\"\"\n",
    "  return data + [' '.join([data[idx] for idx in idx_list])]\n",
    "\n",
    "\n",
    "def is_null_location(row):\n",
    "  \"\"\"Check if any of the location coordinates is zero.\"\"\"\n",
    "  prod = 1.\n",
    "  prod *= float(row[5])\n",
    "  prod *= float(row[6])\n",
    "  prod *= float(row[7])\n",
    "  prod *= float(row[8])\n",
    "  return prod != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to run our pipeline of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data.. Done\n",
      "Preprocessing Writing to /content/preprocessing_demo/geolocation/features_train\n",
      "Writing to /content/preprocessing_demo/geolocation/features_predict\n",
      "Metadata Info\n",
      "['stats', 'features', 'columns']\n",
      "['Trip_distance', 'dropoff', 'pickup', 'Passenger_count', 'Tip_amount', 'Total_amount']\n",
      "['reverse-geo-code', 'count', 'name', 'area', 'min_lat', 'transform', 'max_lon', 'max_lat', 'min_lon', 'type', 'polygons_dict']\n",
      "min lat: (-74.07369232177734, 40.64374923706055) max_lat: (-73.74129486083984, 40.65449142456055)\n",
      "['dropoff', 'pickup', 'target', 'other_data']\n",
      "Done\n",
      "features {\n",
      "  feature {\n",
      "    key: \"dropoff\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"MN13\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"other_data\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -1.0\n",
      "        value: -0.446228712797\n",
      "        value: -0.442896932364\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"BK73\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"target\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 23.5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"dropoff\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"MN13\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"other_data\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -1.0\n",
      "        value: -0.552311420441\n",
      "        value: -0.0250696372241\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"pickup\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"BK73\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"target\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 22.75\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Preparing data..',\n",
    "training_path, testing_path = prepare_data(\n",
    "  '/content/preprocessing_demo/geolocation/ny_taxi_sample.csv')\n",
    "\n",
    "print 'Done\\nPreprocessing',\n",
    "\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "meta, train, test = preprocess(pipeline, training_path, testing_path)\n",
    "pipeline.run()\n",
    "print 'Done'\n",
    "_ = print_results(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
