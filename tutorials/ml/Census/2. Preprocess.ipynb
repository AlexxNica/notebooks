{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "Once you have gathered your data and decided how to preprocess them (a featureset class is already defined), we can preprocess the data. One way to preprocess the data is to use DataFlow. If your data is large, DataFlow can run in cloud in a distributed fashion. If not large, you can also run the DataFlow locally. <br><br>\n",
    "\n",
    "CloudML provides a preprocess DataFlow transformation so it can be easily plugged into the pipeline.\n",
    "\n",
    "What Datalab provides is generated code template with \"%ml preprocess\" command, so you don't have to start from scratch to author your DataFlow pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing requires a featureset class. We've done that in previous \"1.Feature\" notebook but we need to define it again here in this notebook scope.\n",
    "Note that we choose to preprocess all numeric feature columns with [-1, 1] scale by removing the .identity() transform so it uses default transform (scaling to [-1, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.cloud.ml.features as features\n",
    "\n",
    "class CensusFeatures(object):\n",
    "  \"\"\"This class is generated from command line:\n",
    "        %ml features\n",
    "        path: /content/datalab/ml/census/data_train.csv\n",
    "        headers: SERIALNO,PUMA,NP,ACCESS,ACR,AGS,BATH,BDSP,BLD,BROADBND,BUS,COMPOTHX,CONP,DIALUP,DSL,ELEP,FIBEROP,FS,FULP,GASP,HANDHELD,HFL,INSP,LAPTOP,MHP,MODEM,MRGI,MRGP,MRGT,MRGX,OTHSVCEX,REFR,RMSP,RNTM,RNTP,RWAT,SATELLITE,SINK,SMP,STOV,TEL,TEN,TOIL,VALP,VEH,WATP,YBL,FES,FPARC,GRNTP,HHL,HHT,HINCP,HUGCL,HUPAC,HUPAOC,HUPARC,KIT,LNGI,MULTG,MV,NOC,NPF,NPP,NR,NRC,PARTNER,PLM,PSF,R18,R60,R65,RESMODE,SMOCP,SMX,SRNT,SSMC,SVAL,TAXP,WIF,WKEXREL,WORKSTAT\n",
    "        target: HINCP\n",
    "        id: SERIALNO\n",
    "        Please modify it as appropriate!!!\n",
    "  \"\"\"\n",
    "  csv_columns = ('SERIALNO','PUMA','NP','ACCESS','ACR','AGS','BATH','BDSP','BLD','BROADBND','BUS','COMPOTHX','CONP','DIALUP','DSL','ELEP','FIBEROP','FS','FULP','GASP','HANDHELD','HFL','INSP','LAPTOP','MHP','MODEM','MRGI','MRGP','MRGT','MRGX','OTHSVCEX','REFR','RMSP','RNTM','RNTP','RWAT','SATELLITE','SINK','SMP','STOV','TEL','TEN','TOIL','VALP','VEH','WATP','YBL','FES','FPARC','GRNTP','HHL','HHT','HINCP','HUGCL','HUPAC','HUPAOC','HUPARC','KIT','LNGI','MULTG','MV','NOC','NPF','NPP','NR','NRC','PARTNER','PLM','PSF','R18','R60','R65','RESMODE','SMOCP','SMX','SRNT','SSMC','SVAL','TAXP','WIF','WKEXREL','WORKSTAT')\n",
    "  target = features.target('HINCP').regression()\n",
    "  key = features.key('SERIALNO')\n",
    "  inputs = [\n",
    "      features.numeric('CONP'),\n",
    "      features.numeric('WATP'),\n",
    "      features.numeric('FS'),\n",
    "      features.numeric('SMX'),\n",
    "      features.numeric('PSF'),\n",
    "      features.numeric('STOV'),\n",
    "      features.numeric('MULTG'),\n",
    "      features.numeric('WKEXREL'),\n",
    "      features.numeric('BATH'),\n",
    "      features.numeric('INSP'),\n",
    "      features.numeric('ACR'),\n",
    "      features.numeric('NPF'),\n",
    "      features.numeric('YBL'),\n",
    "      features.numeric('HFL'),\n",
    "      features.numeric('TAXP'),\n",
    "      features.numeric('GASP'),\n",
    "      features.numeric('GRNTP'),\n",
    "      features.numeric('MODEM'),\n",
    "      features.numeric('AGS'),\n",
    "      features.numeric('FIBEROP'),\n",
    "      features.numeric('RESMODE'),\n",
    "      features.numeric('SATELLITE'),\n",
    "      features.numeric('DIALUP'),\n",
    "      features.numeric('TEL'),\n",
    "      features.numeric('TEN'),\n",
    "      features.numeric('R18'),\n",
    "      features.numeric('BUS'),\n",
    "      features.numeric('HUPAC'),\n",
    "      features.numeric('SMOCP'),\n",
    "      features.numeric('HANDHELD'),\n",
    "      features.numeric('HUPARC'),\n",
    "      features.numeric('ELEP'),\n",
    "      features.numeric('RMSP'),\n",
    "      features.numeric('R60'),\n",
    "      features.numeric('VEH'),\n",
    "      features.numeric('NP'),\n",
    "      features.numeric('NR'),\n",
    "      features.numeric('SRNT'),\n",
    "      features.numeric('RNTM'),\n",
    "      features.numeric('OTHSVCEX'),\n",
    "      features.numeric('RNTP'),\n",
    "      features.numeric('MRGI'),\n",
    "      features.numeric('WIF'),\n",
    "      features.numeric('LAPTOP'),\n",
    "      features.numeric('REFR'),\n",
    "      features.numeric('TOIL'),\n",
    "      features.numeric('DSL'),\n",
    "      features.numeric('FPARC'),\n",
    "      features.numeric('MRGX'),\n",
    "      features.numeric('FES'),\n",
    "      features.numeric('HHT'),\n",
    "      features.numeric('MRGT'),\n",
    "      features.numeric('BLD'),\n",
    "      features.numeric('SMP'),\n",
    "      features.numeric('MRGP'),\n",
    "      features.numeric('WORKSTAT'),\n",
    "      features.numeric('MHP'),\n",
    "      features.numeric('FULP'),\n",
    "      features.numeric('HUGCL'),\n",
    "      features.numeric('SSMC'),\n",
    "      features.numeric('PUMA'),\n",
    "      features.numeric('LNGI'),\n",
    "      features.numeric('VALP'),\n",
    "      features.numeric('NRC'),\n",
    "      features.numeric('BDSP'),\n",
    "      features.numeric('HUPAOC'),\n",
    "      features.numeric('KIT'),\n",
    "      features.numeric('ACCESS'),\n",
    "      features.numeric('R65'),\n",
    "      features.numeric('NOC'),\n",
    "      features.numeric('MV'),\n",
    "      features.numeric('COMPOTHX'),\n",
    "      features.numeric('SVAL'),\n",
    "      features.numeric('RWAT'),\n",
    "      features.numeric('BROADBND'),\n",
    "      features.numeric('PARTNER'),\n",
    "      features.numeric('PLM'),\n",
    "      features.numeric('HHL'),\n",
    "      features.numeric('NPP'),\n",
    "      features.numeric('SINK'),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run %preprocess, and it generates the input cell for you to fill out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ml preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the cell input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml preprocess\n",
    "train_data_path: /content/datalab/ml/census/data_train.csv\n",
    "eval_data_path: /content/datalab/ml/census/data_eval.csv\n",
    "data_format: CSV\n",
    "output_dir: /content/datalab/ml/census/preprocessed\n",
    "feature_set_class_name: CensusFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Preprocessing\n",
    "\n",
    "Run it and below is what you get. You can run the pipeline directly (it is a local pipeline), or extend it with more DataFlow transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.direct_runner.DirectPipelineResult at 0x7fc4b50f89d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml preprocess\n",
    "train_data_path: /content/datalab/ml/census/data_train.csv\n",
    "eval_data_path: /content/datalab/ml/census/data_eval.csv\n",
    "data_format: CSV\n",
    "output_dir: /content/datalab/ml/census/preprocessed\n",
    "feature_set_class_name: CensusFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = CensusFeatures()\n",
    "OUTPUT_DIR = '/content/datalab/ml/census/preprocessed'\n",
    "pipeline = beam.Pipeline('DirectPipelineRunner')\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    '/content/datalab/ml/census/data_train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    '/content/datalab/ml/census/data_eval.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "metadata | io.SaveMetadata(os.path.join(OUTPUT_DIR, \"metadata.yaml\"))\n",
    "train_features | beam.Write('WriteTraining', beam.io.TextFileSink(\n",
    "    os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "\n",
    "eval_features | beam.Write('WriteEval', beam.io.TextFileSink(\n",
    "    os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_eval-00000-of-00001  features_train-00000-of-00001  metadata.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/datalab/ml/census/preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Preprocessing\n",
    "You can also generate Cloud DataFlow pipeline. Just add \"--cloud\" to \"%ml preprocess\". <br>\n",
    "Note that if you need to get it running in cloud, you need: <br>\n",
    "1. Sign In using the up right sign-in button, if you have not done so. <br>\n",
    "2. Set a default project by running '%projects set Your-Project-Id'.\n",
    "3. Your data need to be in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%projects set cloud-ml-test-automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%storage create --bucket gs://cloud-ml-test-automated-sampledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying file:///content/datalab/ml/census/data_train.csv [Content-Type=text/csv]...\n",
      "Uploading   ...-test-automated-sampledata/census/data_train.csv: 4.24 MiB/4.24 MiB    \n",
      "Copying file:///content/datalab/ml/census/data_eval.csv [Content-Type=text/csv]...\n",
      "Uploading   ...l-test-automated-sampledata/census/data_eval.csv: 482.12 KiB/482.12 KiB    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/datalab/ml/census/data_train.csv gs://cloud-ml-test-automated-sampledata/census/data_train.csv \n",
    "!gsutil cp /content/datalab/ml/census/data_eval.csv gs://cloud-ml-test-automated-sampledata/census/data_eval.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the input before it generates the cloud pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml preprocess --cloud\n",
    "train_data_path: gs://cloud-ml-test-automated-sampledata/census/data_train.csv \n",
    "eval_data_path: gs://cloud-ml-test-automated-sampledata/census/data_eval.csv \n",
    "data_format: CSV\n",
    "output_dir: gs://cloud-ml-test-automated-sampledata/census/preprocessed\n",
    "feature_set_class_name: CensusFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the following generated code, you can go to Developer Console to see the DataFlow job. For example, https://pantheon.corp.google.com/dataflow?project=cloud-ml-test-automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using fallback coder for typehint: Dict[Any, Any].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DataflowPipelineResult <Job\n",
       " id: u'2016-08-18_22_03_30-10137579761566618208'\n",
       " projectId: u'cloud-ml-test-automated'\n",
       " steps: []\n",
       " tempFiles: []\n",
       " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)> at 0x7f257b2dce50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# header\n",
    "\"\"\"\n",
    "Following code is generated from command line:\n",
    "%%ml preprocess --cloud\n",
    "train_data_path: gs://cloud-ml-test-automated-sampledata/census/data_train.csv \n",
    "eval_data_path: gs://cloud-ml-test-automated-sampledata/census/data_eval.csv \n",
    "data_format: CSV\n",
    "output_dir: gs://cloud-ml-test-automated-sampledata/census/preprocessed\n",
    "feature_set_class_name: CensusFeatures\n",
    "\n",
    "Please modify as appropriate!!!\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import apache_beam as beam\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io\n",
    "import os\n",
    "\n",
    "# defines\n",
    "feature_set = CensusFeatures()\n",
    "OUTPUT_DIR = 'gs://cloud-ml-test-automated-sampledata/census/preprocessed'\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-censusfeatures-160819-050307',\n",
    "    'project': 'cloud-ml-test-automated',\n",
    "    'extra_packages': ['gs://cloud-datalab/deploy/cloudml.tar.gz'],\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline('DataflowPipelineRunner', options=opts)\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "training_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-ml-test-automated-sampledata/census/data_train.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "train = pipeline | beam.Read('ReadTrainingData', training_data)\n",
    "eval_data = beam.io.TextFileSource(\n",
    "    'gs://cloud-ml-test-automated-sampledata/census/data_eval.csv',\n",
    "    strip_trailing_newlines=True,\n",
    "    coder=io.CsvCoder.from_feature_set(feature_set, feature_set.csv_columns))\n",
    "eval = pipeline | beam.Read('ReadEvalData', eval_data)\n",
    "(metadata, train_features, eval_features) = ((train, eval) |\n",
    "    ml.Preprocess('Preprocess', feature_set))\n",
    "metadata | io.SaveMetadata(os.path.join(OUTPUT_DIR, \"metadata.yaml\"))\n",
    "train_features | beam.Write('WriteTraining', beam.io.TextFileSink(\n",
    "    os.path.join(OUTPUT_DIR, 'features_train')))\n",
    "\n",
    "eval_features | beam.Write('WriteEval', beam.io.TextFileSink(\n",
    "    os.path.join(OUTPUT_DIR, 'features_eval')))\n",
    "\n",
    "# run pipeline\n",
    "pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00000-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00001-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00002-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00003-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00004-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00005-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00006-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00007-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval-00008-of-00009\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00000-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00001-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00002-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00003-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00004-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00005-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00006-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train-00007-of-00008\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/metadata.yaml\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/preprocessed/tmp/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil list gs://cloud-ml-test-automated-sampledata/census/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
