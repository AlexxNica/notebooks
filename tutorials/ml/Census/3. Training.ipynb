{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training requires a tarball python package that includes your training program based on TensorFlow. While CloudML provides several generic purpose model training, for this sample we will use a package that is specifically created to train Census sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Package\n",
    "\n",
    "You can use existing tarball package (locally or in GCS), or use your own tarball package. You can define a python module use \"%%ml module\". In the following two cells, we will define two python modules: \"census\" and \"task\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml module --name census\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "import json\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_examples(input_files, batch_size, shuffle, num_epochs=None):\n",
    "  # The minimum number of instances in a queue from which examples are drawn\n",
    "  # randomly. The larger this number, the more randomness at the expense of\n",
    "  # higher memory requirements.\n",
    "  MIN_AFTER_DEQUEUE = 100\n",
    "\n",
    "  # When batching data, the queue's capacity will be larger than the batch_size\n",
    "  # by some factor. The recommended formula is (num_threads + a small safety\n",
    "  # margin). For now, we use a single thread for reading, so this can be small.\n",
    "  QUEUE_SIZE_MULTIPLIER = 3\n",
    "\n",
    "  # Convert num_epochs == 0 -> num_epochs is None, if necessary\n",
    "  num_epochs = num_epochs or None\n",
    "\n",
    "  # Build a queue of the filenames to be read.\n",
    "  filename_queue = tf.train.string_input_producer(input_files, num_epochs,\n",
    "                                                  shuffle)\n",
    "  options = tf.python_io.TFRecordOptions(\n",
    "      compression_type=tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "  example_id, encoded_example = tf.TFRecordReader(options=options).read(\n",
    "      filename_queue)\n",
    "\n",
    "  if shuffle:\n",
    "    capacity = MIN_AFTER_DEQUEUE + QUEUE_SIZE_MULTIPLIER * batch_size\n",
    "    return tf.train.shuffle_batch([example_id, encoded_example], batch_size,\n",
    "                                  capacity, MIN_AFTER_DEQUEUE)\n",
    "  else:\n",
    "    capacity = QUEUE_SIZE_MULTIPLIER * batch_size\n",
    "    return tf.train.batch([example_id, encoded_example],\n",
    "                          batch_size,\n",
    "                          capacity=capacity)\n",
    "\n",
    "def create_inputs(metadata, input_data=None):\n",
    "  with tf.name_scope('inputs'):\n",
    "    if input_data is None:\n",
    "      input_data = tf.placeholder(tf.string, name='input', shape=(None,))\n",
    "    parsed = features.FeatureMetadata.parse_features(metadata, input_data)\n",
    "    return (input_data, parsed['inputs'], tf.squeeze(parsed['target']),\n",
    "            tf.identity(parsed['key']))\n",
    "\n",
    "def _create_layer(inputs, input_size, output_size):\n",
    "  with tf.name_scope('layer'):\n",
    "    initial_weights = tf.truncated_normal([input_size, output_size],\n",
    "                                          stddev = 1.0 / math.sqrt(input_size))\n",
    "    weights = tf.Variable(initial_weights, name = 'weights')\n",
    "\n",
    "    initial_biases = tf.zeros([ output_size ])\n",
    "    biases = tf.Variable(initial_biases, name = 'biases')\n",
    "\n",
    "    xw = tf.matmul(inputs, weights)\n",
    "\n",
    "    return tf.nn.bias_add(xw, biases)\n",
    "\n",
    "def inference(inputs, metadata, hyperparams):\n",
    "  input_size = metadata.features['inputs']['size']\n",
    "  output_size = metadata.features['target']['size']\n",
    "\n",
    "  hidden_layer1 = tf.nn.relu(_create_layer(inputs, input_size,\n",
    "                                           hyperparams['hidden_layer1_size']))\n",
    "  hidden_layer2 = tf.nn.relu(_create_layer(hidden_layer1,\n",
    "                                           hyperparams['hidden_layer1_size'],\n",
    "                                           hyperparams['hidden_layer2_size']))\n",
    "  hidden_layer3 = tf.nn.relu(_create_layer(hidden_layer2,\n",
    "                                           hyperparams['hidden_layer2_size'],\n",
    "                                           hyperparams['hidden_layer3_size']))\n",
    "  output = _create_layer(hidden_layer3, hyperparams['hidden_layer3_size'],\n",
    "                         output_size)\n",
    "  return output\n",
    "\n",
    "\n",
    "def loss(output, targets):\n",
    "  \"\"\"Calculates the loss from the output and the labels.\n",
    "  Args:\n",
    "    output: output layer tensor, float - [batch_size].\n",
    "    targets: Target value tensor, float - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  loss = tf.reduce_mean(tf.abs(output - targets), name = 'loss')\n",
    "  return loss\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  with tf.name_scope('train'):\n",
    "    tf.scalar_summary(loss.op.name, loss)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step)\n",
    "    return train_op, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml module --name task --main\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import census\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.metrics.python.ops import metric_ops\n",
    "\n",
    "import google.cloud.ml.features as features\n",
    "import google.cloud.ml.util as cloudml_util\n",
    "\n",
    "EXPORT_SUBDIRECTORY = 'model'\n",
    "HYPERPARAMS = {\n",
    "  'batch_size': 64,\n",
    "  'learning_rate': 0.003,\n",
    "}\n",
    "EVAL_SET_SIZE = 2767\n",
    "EVAL_INTERVAL_SECS = 15\n",
    "\n",
    "\n",
    "def print_to_console(msg):\n",
    "  print msg\n",
    "  sys.stdout.flush()\n",
    "\n",
    "\n",
    "def main():\n",
    "  config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  cluster = config.get('cluster', None)\n",
    "  task = config.get('task', None)\n",
    "  job = config.get('job', None)\n",
    "  trial_id = task.get('trial', '')\n",
    "  logging.info(\"start trial %s.\", trial_id)\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--train_data_paths\", type=str, action='append')\n",
    "  parser.add_argument(\"--eval_data_paths\", type=str, action='append')\n",
    "  parser.add_argument(\"--metadata_path\", type=str)\n",
    "  parser.add_argument(\"--output_path\", type=str)\n",
    "  parser.add_argument(\"--max_steps\", type=int, default=2000)\n",
    "  parser.add_argument(\"--hidden1\", type=int, default=300)\n",
    "  parser.add_argument(\"--hidden2\", type=int, default=200)\n",
    "  parser.add_argument(\"--hidden3\", type=int, default=100)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  dispatch(args, cluster, task, job, trial_id)\n",
    "\n",
    "\n",
    "def start_server(cluster, task):\n",
    "  # Create and start a server.\n",
    "  return tf.train.Server(cluster,\n",
    "                         protocol=\"grpc\",\n",
    "                         job_name=task['type'],\n",
    "                         task_index=task['index'])\n",
    "\n",
    "\n",
    "def dispatch(args, cluster, task, job, trial_id):\n",
    "  if not cluster:\n",
    "    # Run locally.\n",
    "    run_training(args, target=\"\", is_chief=True, device_fn=\"\", trial_id=trial_id)\n",
    "    return\n",
    "\n",
    "  if task['type'] == \"ps\":\n",
    "    server = start_server(cluster, task)\n",
    "    server.join()\n",
    "  elif task['type'] == \"worker\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = False\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  elif task['type'] == \"master\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = (task['index'] == 0)\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:master/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  else:\n",
    "    raise ValueError(\"invalid job_type %s\" % task['type'])\n",
    "\n",
    "\n",
    "def run_training(args, target, is_chief, device_fn, trial_id):\n",
    "  \"\"\"Train Census for a number of steps.\"\"\"\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  # Get the sets of examples and targets for training, validation, and\n",
    "  # test on Census.\n",
    "  training_data = args.train_data_paths\n",
    "\n",
    "  if is_chief:\n",
    "    # A generator over accuracies. Each call to next(accuracies) forces an\n",
    "    # evaluation of the model.\n",
    "    accuracies = evaluate(args, trial_id)\n",
    "\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "  with tf.Graph().as_default() as graph:\n",
    "    # Assigns ops to the local worker by default.\n",
    "    with tf.device(device_fn):\n",
    "\n",
    "      metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "\n",
    "      _, train_examples = census.read_examples(\n",
    "          training_data, HYPERPARAMS['batch_size'], shuffle=False)\n",
    "\n",
    "      # Generate placeholders for the examples.\n",
    "      placeholder, inputs, targets, _ = (\n",
    "          census.create_inputs(metadata, input_data=train_examples))\n",
    "\n",
    "      # Build a Graph that computes predictions from the inference model.\n",
    "      layer_sizes = {\n",
    "        'hidden_layer1_size': args.hidden1,\n",
    "        'hidden_layer2_size': args.hidden2,\n",
    "        'hidden_layer3_size': args.hidden3,\n",
    "      }\n",
    "      output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "      # Add to the Graph the Ops for loss calculation.\n",
    "      loss = census.loss(output, targets)\n",
    "\n",
    "      # Add to the Graph the Ops that calculate and apply gradients.\n",
    "      train_op, global_step = census.training(loss,\n",
    "                                              HYPERPARAMS['learning_rate'])\n",
    "\n",
    "      # Build the summary operation based on the TF collection of Summaries.\n",
    "      summary_op = tf.merge_all_summaries()\n",
    "\n",
    "      # Add the variable initializer Op.\n",
    "      init_op = tf.initialize_all_variables()\n",
    "\n",
    "      # Create a saver for writing training checkpoints.\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "      summary_writer = tf.train.SummaryWriter(os.path.join(\n",
    "          output_path, 'summaries'), graph)\n",
    "\n",
    "      # Create a \"supervisor\", which oversees the training process.\n",
    "      sv = tf.train.Supervisor(is_chief=is_chief,\n",
    "                               logdir=os.path.join(output_path, 'logdir'),\n",
    "                               init_op=init_op,\n",
    "                               saver=saver,\n",
    "                               summary_op=None,\n",
    "                               global_step=global_step,\n",
    "                               save_model_secs=60)\n",
    "\n",
    "      # The supervisor takes care of session initialization, restoring from\n",
    "      # a checkpoint, and closing when done or an error occurs.\n",
    "      print_to_console(\"Starting the loop.\")\n",
    "      with sv.managed_session(target) as sess:\n",
    "        start_time = time.time()\n",
    "        last_save = start_time\n",
    "\n",
    "        # Loop until the supervisor shuts down or max_steps have completed.\n",
    "        step = 0\n",
    "        while not sv.should_stop() and step < args.max_steps:\n",
    "          start_time = time.time()\n",
    "\n",
    "          # Run one step of the model.  The return values are the activations\n",
    "          # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "          # inspect the values of your Ops or variables, you may include them\n",
    "          # in the list passed to sess.run() and the value tensors will be\n",
    "          # returned in the tuple from the call.\n",
    "          _, step, loss_value = sess.run([train_op, global_step, loss])\n",
    "\n",
    "          duration = time.time() - start_time\n",
    "          if is_chief and time.time() - last_save > EVAL_INTERVAL_SECS:\n",
    "            last_save = time.time()\n",
    "            saver.save(sess, sv.save_path, global_step)\n",
    "            accuracy = next(accuracies)\n",
    "            logging.info(\"Eval, step %d: error = %0.3f\", step, accuracy)\n",
    "            print_to_console(\"Eval, step %d: error = %0.3f\" % (step, accuracy))\n",
    "\n",
    "          # Write the summaries and log an overview fairly often.\n",
    "          if step % 200 == 0 and is_chief:\n",
    "            logging.info(\"Step %d: loss = %.2f (%.3f sec)\",\n",
    "                         step, loss_value, duration)\n",
    "            print_to_console(\"Step %d: loss = %.2f (%.3f sec)\" % (step, loss_value, duration))\n",
    "\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "\n",
    "        if is_chief:\n",
    "          # Force a save at the end of our loop.\n",
    "          sv.saver.save(sess, sv.save_path, global_step=global_step,\n",
    "                        write_meta_graph=False)\n",
    "          accuracy_value = next(accuracies)\n",
    "          logging.info(\"Final error after %d steps = %0.3f\", step, accuracy_value)\n",
    "          print_to_console(\"Final error after %d steps = %0.3f\" % (step, accuracy_value))\n",
    "\n",
    "          # Save the model for inference\n",
    "          export_model(args, sess, sv.saver, trial_id)\n",
    "\n",
    "      # Ask for all the services to stop.\n",
    "      sv.stop()\n",
    "      print_to_console(\"Done training.\")\n",
    "\n",
    "\n",
    "def export_model(args, sess, training_saver, trial_id):\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  with tf.Graph().as_default() as inference_graph:\n",
    "    metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "    placeholder, inputs, _, keys = census.create_inputs(metadata)\n",
    "    layer_sizes = {\n",
    "      'hidden_layer1_size': args.hidden1,\n",
    "      'hidden_layer2_size': args.hidden2,\n",
    "      'hidden_layer3_size': args.hidden3,\n",
    "    }\n",
    "    output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "    inference_saver = tf.train.Saver()\n",
    "\n",
    "    # Mark the inputs and the outputs\n",
    "    tf.add_to_collection(\"inputs\",\n",
    "                         json.dumps({\"examples\": placeholder.name}))\n",
    "    tf.add_to_collection(\"outputs\",\n",
    "                         json.dumps({\"score\": output.name}))\n",
    "    tf.add_to_collection(\"keys\", json.dumps({\"key\": keys.name}))\n",
    "\n",
    "    model_dir = os.path.join(output_path, EXPORT_SUBDIRECTORY)\n",
    "\n",
    "    # Save a copy of the metadata file used for this model with the exported\n",
    "    # model, so that online and batch prediction can use it.\n",
    "    subprocess.check_call(['gsutil', 'cp', args.metadata_path,\n",
    "                           os.path.join(model_dir, \"metadata.yaml\")])\n",
    "    \n",
    "    inference_saver.export_meta_graph(\n",
    "        filename=os.path.join(model_dir, \"export.meta\"))\n",
    "\n",
    "    # Save the variables. Don't write the MetaGraphDef, because that is\n",
    "    # actually the training graph.\n",
    "    training_saver.save(sess,\n",
    "                        os.path.join(model_dir, \"export\"),\n",
    "                        write_meta_graph=False)\n",
    "\n",
    "\n",
    "def evaluate(args, trial_id):\n",
    "  \"\"\"Run one round of evaluation, yielding accuracy.\"\"\"\n",
    "  output_path = os.path.join(args.output_path, trial_id)\n",
    "  eval_data = args.eval_data_paths\n",
    "\n",
    "  with tf.Graph().as_default() as g:\n",
    "    metadata = features.FeatureMetadata.get_metadata(args.metadata_path)\n",
    "\n",
    "    _, examples = census.read_examples(\n",
    "        eval_data, HYPERPARAMS['batch_size'],\n",
    "        shuffle=False)\n",
    "\n",
    "    # Generate placeholders for the examples.\n",
    "    placeholder, inputs, targets, _ = (\n",
    "        census.create_inputs(metadata, input_data=examples))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    layer_sizes = {\n",
    "      'hidden_layer1_size': args.hidden1,\n",
    "      'hidden_layer2_size': args.hidden2,\n",
    "      'hidden_layer3_size': args.hidden3,\n",
    "    }\n",
    "    output = census.inference(inputs, metadata, layer_sizes)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss = census.loss(output, targets)\n",
    "\n",
    "    # Add the Op to compute accuracy.\n",
    "    error, eval_op = metric_ops.streaming_mean_relative_error(\n",
    "        output, targets, tf.ones(HYPERPARAMS['batch_size']))\n",
    "\n",
    "    # The global step is useful for summaries.\n",
    "    with tf.name_scope('train'):\n",
    "      global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    summary = tf.scalar_summary(\"error\", error)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "  num_eval_batches = float(EVAL_SET_SIZE) // HYPERPARAMS['batch_size']\n",
    "  summary_writer = tf.train.SummaryWriter(os.path.join(\n",
    "      output_path, 'eval'))\n",
    "\n",
    "  sv = tf.train.Supervisor(graph=g,\n",
    "                           logdir=os.path.join(output_path, 'eval'),\n",
    "                           summary_op=summary,\n",
    "                           summary_writer=summary_writer,\n",
    "                           global_step=None,\n",
    "                           saver=saver)\n",
    "\n",
    "  step = 0\n",
    "  while step < args.max_steps:\n",
    "    last_checkpoint = tf.train.latest_checkpoint(os.path.join(\n",
    "        output_path, 'logdir'))\n",
    "    with sv.managed_session(master=\"\",\n",
    "                            start_standard_services=False) as session:\n",
    "      sv.start_queue_runners(session)\n",
    "      sv.saver.restore(session, last_checkpoint)\n",
    "      accuracy = tf_evaluation(session,\n",
    "                               max_num_evals=num_eval_batches,\n",
    "                               eval_op=eval_op,\n",
    "                               final_op=error,\n",
    "                               summary_op=summary,\n",
    "                               summary_writer=summary_writer,\n",
    "                               global_step=global_step)\n",
    "\n",
    "      step = tf.train.global_step(session, global_step)\n",
    "      yield accuracy\n",
    "\n",
    "\n",
    "def tf_evaluation(sess,\n",
    "                  max_num_evals=1000,\n",
    "                  eval_op=None,\n",
    "                  final_op=None,\n",
    "                  summary_op=None,\n",
    "                  summary_writer=None,\n",
    "                  global_step=None):\n",
    "  if eval_op is not None:\n",
    "    try:\n",
    "      for i in range(int(max_num_evals)):\n",
    "        (_, final_op_value) = sess.run((eval_op, final_op))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      # We've hit the end of our epoch.  Unfortunately, if we hit this\n",
    "      # tensorflow has already logged a warning to stderr, so we try to avoid\n",
    "      # hitting it in this sample.\n",
    "      pass\n",
    "\n",
    "  if summary_op is not None:\n",
    "    if global_step is None:\n",
    "      raise ValueError(\"must specify global step\")\n",
    "\n",
    "    global_step = tf.train.global_step(sess, global_step)\n",
    "    summary = sess.run(summary_op)\n",
    "    hptuning_summary = tf.Summary(value=[\n",
    "      tf.Summary.Value(tag='training/hptuning/metric', simple_value=float(final_op_value))\n",
    "    ])\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "    summary_writer.add_summary(hptuning_summary, global_step)\n",
    "    summary_writer.flush()\n",
    "\n",
    "  return final_op_value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"%ml train\" to generate the training cell template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%ml train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the required fields and run. <br>\n",
    "Datalab will simulate the CloudML service by creating master, worker, and ps processes (in cloud they are different VMs) to perform a distributed training, although all these processes run in the local container VM.<br>\n",
    "You can set replica_count to 0 to not using a certain job type, such as ps. But master is required. In this case, we only enable master.<br>\n",
    "The output of the training will be links to the processes output logs, and also refreshed every 3 seconds to show last few lines of the logs. You can use the local run to quickly validate your training program and parameters before submitting it to cloud to do large scale training.<br>\n",
    "If for any reasons the training is stuck, just click \"Reset Session\" to reset the kernel. All training processes will be cleaned up.<br><br>\n",
    "\n",
    "There are two ways you could specify a trainer program: you can specify \"package_uris\" and \"python_module\" in the input cell for existing tarball package. Or, if these are absent, it will look for all \"%%ml module\" cells and create a temp tarball package to run. <br>\n",
    "\n",
    "Since we already defined our training modules, let's run the training program without explicitly specifying package. Datalab will create a temp package and will run the entrypoint module specified by \"--main\" flag. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/worker\" target=\"_blank\">worker log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;<a href=\"/_nocachecontent/ps\" target=\"_blank\">ps log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: Final error after 2000 steps = 48704.273<br/>master: Copying file:///content/datalab/tmp/ml/census/preprocessed/metadata.yaml...<br/>master: / [0 files][    0.0 B/ 16.1 KiB]                                                <br/>master: / [1 files][ 16.1 KiB/ 16.1 KiB]                                                <br/>master: Operation completed over 1 objects/16.1 KiB.                                     <br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: <br/>worker: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>worker: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:worker/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>worker: Done training.<br/>worker: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml train\n",
    "worker_count: 1\n",
    "parameter_server_count: 1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/datalab/tmp/ml/census/preprocessed/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/datalab/tmp/ml/census/preprocessed/features_eval\n",
    "  metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/census/model\n",
    "  hidden1: 100\n",
    "  hidden2: 60\n",
    "  hidden3: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval  logdir  model  summaries\r\n"
     ]
    }
   ],
   "source": [
    "!ls /content/datalab/tmp/ml/census/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start TensorBoard to view training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 115536. Click <a href=\"/_proxy/34267/\" target=\"_blank\">here</a> to access it.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorboard start --logdir /content/datalab/tmp/ml/census/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the tensorboard serverwhen you are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tensorboard stop --pid 115536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train another model with larger hidden layer sizes.\n",
    "Instead of running the modules defined by \"%%ml modules\" directly, we will package the modules first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package created at /content/datalab/tmp/ml/census/trainer-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "%%ml package --out /content/datalab/tmp/ml/census/ --name trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the package explicitly by package_uris. Since we don't specify 'parameter_server_count' or 'worker_count', we will use one master only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: Step 2000: loss = 46214.98 (0.045 sec)<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: Final error after 2000 steps = 49071.629<br/>master: Copying file:///content/datalab/tmp/ml/census/preprocessed/metadata.yaml...<br/>master: / [0 files][    0.0 B/ 16.1 KiB]                                                <br/>master: / [1 files][ 16.1 KiB/ 16.1 KiB]                                                <br/>master: Operation completed over 1 objects/16.1 KiB.                                     <br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:master/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: Done training.<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]<br/>master: E tensorflow/core/client/tensor_c_api.cc:485] Enqueue operation was cancelled<br/>master: \t [[Node: batch/fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_STRING, DT_STRING], _class=[\"loc:@batch/fifo_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, ReaderRead, ReaderRead:1)]]<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ml train\n",
    "package_uris: /content/datalab/tmp/ml/census/trainer-0.1.tar.gz\n",
    "python_module: trainer.task\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - /content/datalab/tmp/ml/census/preprocessed/features_train\n",
    "  eval_data_paths:\n",
    "    - /content/datalab/tmp/ml/census/preprocessed/features_eval\n",
    "  metadata_path: /content/datalab/tmp/ml/census/preprocessed/metadata.yaml\n",
    "  output_path: /content/datalab/tmp/ml/census/largermodel\n",
    "  hidden1: 200\n",
    "  hidden2: 100\n",
    "  hidden3: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Training\n",
    "\n",
    "Cloud training is similar but with \"--cloud\" flag, and use all GCS paths instead of local paths. <br>\n",
    "You also need to make sure you have a project whitelisted for CloudML, and use \"%projects set project-id\" to set it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = 'gs://' + datalab_project_id() + '-sampledata'\n",
    "package_path = os.path.join(bucket, 'census', 'model', 'trainer-0.1.tar.gz')\n",
    "train_data_path = os.path.join(bucket, 'census', 'preprocessed', 'features_train')\n",
    "eval_data_path = os.path.join(bucket, 'census', 'preprocessed', 'features_eval')\n",
    "metadata_path = os.path.join(bucket, 'census', 'preprocessed', 'metadata.yaml')\n",
    "output_path = os.path.join(bucket, 'census', 'trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/tmp/ml/census/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  5.4 KiB/  5.4 KiB]                                                \n",
      "Operation completed over 1 objects/5.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/datalab/tmp/ml/census/trainer-0.1.tar.gz $package_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training using the Cloud DataFlow output from the \"2. Preprocessing\" notebook. We choose a set of hidden layer sizes, and later we will show how to sweep hyperparameter values using CloudML service using hyperparameter tuning feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_160922_012016\" was submitted successfully.<br/>Run \"%ml jobs --name trainer_task_160922_012016\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=cloud-ml-test-automated&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_160922_012016\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml train --cloud\n",
    "package_uris: $package_path\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-west1\n",
    "args:\n",
    "  train_data_paths:\n",
    "    - $train_data_path\n",
    "  eval_data_paths:\n",
    "    - $eval_data_path\n",
    "  metadata_path: $metadata_path\n",
    "  output_path: $output_path\n",
    "  hidden1: 200\n",
    "  hidden2: 100\n",
    "  hidden3: 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the job status as described in the output. You can also run \"%ml jobs --filter state!=SUCCEEDED\" to see all active ML jobs in that project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>createTime: '2016-09-22T01:20:17Z'\n",
       "endTime: '2016-09-22T01:28:33Z'\n",
       "jobId: trainer_task_160922_012016\n",
       "startTime: '2016-09-22T01:23:03Z'\n",
       "state: SUCCEEDED\n",
       "trainingInput:\n",
       "  args: [--hidden3, '50', --hidden2, '100', --hidden1, '200', --output_path, 'gs://cloud-ml-test-automated-sampledata/census/trained',\n",
       "    --train_data_paths, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_train',\n",
       "    --metadata_path, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/metadata.yaml',\n",
       "    --eval_data_paths, 'gs://cloud-ml-test-automated-sampledata/census/preprocessed/features_eval']\n",
       "  packageUris: ['gs://cloud-ml-test-automated-sampledata/census/model/trainer-0.1.tar.gz']\n",
       "  pythonModule: trainer.task\n",
       "  region: us-west1\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ml jobs --name trainer_task_160922_012016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ml-test-automated-sampledata/census/trained/eval/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/logdir/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/model/\r\n",
      "gs://cloud-ml-test-automated-sampledata/census/trained/summaries/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-ml-test-automated-sampledata/census/trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
